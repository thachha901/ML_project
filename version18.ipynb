{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d76982c9",
   "metadata": {
    "papermill": {
     "duration": 0.007306,
     "end_time": "2024-12-19T08:36:52.471570",
     "exception": false,
     "start_time": "2024-12-19T08:36:52.464264",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Import necessary library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4593fc9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T08:36:52.488032Z",
     "iopub.status.busy": "2024-12-19T08:36:52.487206Z",
     "iopub.status.idle": "2024-12-19T08:37:16.589538Z",
     "shell.execute_reply": "2024-12-19T08:37:16.588770Z"
    },
    "papermill": {
     "duration": 24.113563,
     "end_time": "2024-12-19T08:37:16.591675",
     "exception": false,
     "start_time": "2024-12-19T08:36:52.478112",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.optimize import minimize\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import Adam\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from colorama import Fore, Style\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import VotingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "SEED = 42\n",
    "n_splits = 5\n",
    "\n",
    "import optuna\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.linear_model import Ridge\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.cluster import DBSCAN\n",
    "import polars as pl\n",
    "import polars.selectors as cs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e374a9c2",
   "metadata": {
    "papermill": {
     "duration": 0.005398,
     "end_time": "2024-12-19T08:37:16.603177",
     "exception": false,
     "start_time": "2024-12-19T08:37:16.597779",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Processing file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5656195a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T08:37:16.615810Z",
     "iopub.status.busy": "2024-12-19T08:37:16.615119Z",
     "iopub.status.idle": "2024-12-19T08:37:16.621574Z",
     "shell.execute_reply": "2024-12-19T08:37:16.620784Z"
    },
    "papermill": {
     "duration": 0.01452,
     "end_time": "2024-12-19T08:37:16.623186",
     "exception": false,
     "start_time": "2024-12-19T08:37:16.608666",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_file(filename, dirname):\n",
    "    df = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))\n",
    "    df.drop('step', axis=1, inplace=True)\n",
    "    return df.describe().values.reshape(-1), filename.split('=')[1]\n",
    "\n",
    "def load_time_series(dirname) -> pd.DataFrame:\n",
    "    ids = os.listdir(dirname)\n",
    "    \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(tqdm(executor.map(lambda fname: process_file(fname, dirname), ids), total=len(ids)))\n",
    "    \n",
    "    stats, indexes = zip(*results)\n",
    "    \n",
    "    df = pd.DataFrame(stats, columns=[f\"stat_{i}\" for i in range(len(stats[0]))])\n",
    "    df['id'] = indexes\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70475dbe",
   "metadata": {
    "papermill": {
     "duration": 0.005283,
     "end_time": "2024-12-19T08:37:16.633910",
     "exception": false,
     "start_time": "2024-12-19T08:37:16.628627",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2.1 Sparse Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3843c5",
   "metadata": {},
   "source": [
    "A sparse autoencoder was utilized to help the model learn the features better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00e4db8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T08:37:16.646777Z",
     "iopub.status.busy": "2024-12-19T08:37:16.646112Z",
     "iopub.status.idle": "2024-12-19T08:37:16.661846Z",
     "shell.execute_reply": "2024-12-19T08:37:16.661106Z"
    },
    "papermill": {
     "duration": 0.02428,
     "end_time": "2024-12-19T08:37:16.663479",
     "exception": false,
     "start_time": "2024-12-19T08:37:16.639199",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sparse Autoencoder Model\n",
    "class SparseAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, sparsity_weight=1e-5):\n",
    "        super(SparseAutoencoder, self).__init__()\n",
    "        self.sparsity_weight = sparsity_weight\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_dim),\n",
    "            nn.Sigmoid()  # Outputs in the range [0, 1]\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "\n",
    "# Preparing Data\n",
    "# Option to use different scalers: MinMaxScaler, StandardScaler, RobustScaler\n",
    "def prepare_data(data, scaler_type='MinMaxScaler'):\n",
    "    if scaler_type == 'StandardScaler':\n",
    "        scaler = StandardScaler()\n",
    "    elif scaler_type == 'RobustScaler':\n",
    "        scaler = RobustScaler()\n",
    "    else:\n",
    "        scaler = MinMaxScaler()\n",
    "    \n",
    "    data_scaled = scaler.fit_transform(data)\n",
    "    return torch.tensor(data_scaled, dtype=torch.float32), scaler\n",
    "\n",
    "# Apply PCA for Dimensionality Reduction\n",
    "# This can help focus the autoencoder on the most relevant features\n",
    "def apply_pca(data, n_components=0.95):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    data_pca = pca.fit_transform(data)\n",
    "    return data_pca, pca\n",
    "\n",
    "# Early Stopping Functionality\n",
    "def early_stopping(patience):\n",
    "    class EarlyStopping:\n",
    "        def __init__(self, patience=patience):\n",
    "            self.patience = patience\n",
    "            self.counter = 0\n",
    "            self.best_loss = float('inf')\n",
    "            self.early_stop = False\n",
    "        \n",
    "        def __call__(self, loss):\n",
    "            if loss < self.best_loss:\n",
    "                self.best_loss = loss\n",
    "                self.counter = 0\n",
    "            else:\n",
    "                self.counter += 1\n",
    "                if self.counter >= self.patience:\n",
    "                    self.early_stop = True\n",
    "    return EarlyStopping()\n",
    "\n",
    "# Training the Sparse Autoencoder with DataFrame Output\n",
    "def perform_autoencoder(data, epochs=100, batch_size=32, learning_rate=0.001, patience=10, scaler_type='MinMaxScaler', use_pca=False, sparsity_weight=1e-5):\n",
    "    # Preprocess Data\n",
    "    if use_pca:\n",
    "        data, pca = apply_pca(data)\n",
    "\n",
    "    data_tensor, scaler = prepare_data(data, scaler_type=scaler_type)\n",
    "    train_data, val_data = train_test_split(data_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(train_data), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(val_data), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = SparseAutoencoder(input_dim=data.shape[1], sparsity_weight=sparsity_weight)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.SmoothL1Loss()  # Changed to Smooth L1 Loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    stopper = early_stopping(patience=patience)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            batch = batch[0].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            encoded, outputs = model(batch)\n",
    "            \n",
    "            # Reconstruction loss\n",
    "            loss = criterion(outputs, batch)\n",
    "            \n",
    "            # Sparsity penalty (L1 regularization on encoded activations)\n",
    "            l1_penalty = torch.mean(torch.abs(encoded))\n",
    "            loss += sparsity_weight * l1_penalty\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * batch.size(0)\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch[0].to(device)\n",
    "                _, outputs = model(batch)\n",
    "                loss = criterion(outputs, batch)\n",
    "                val_loss += loss.item() * batch.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        stopper(val_loss)\n",
    "        if stopper.early_stop:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    # Convert tensor back to DataFrame for consistency\n",
    "    _, data_decoded = model(data_tensor.to(device))\n",
    "    data_decoded = data_decoded.cpu().detach().numpy()\n",
    "    df_encoded = pd.DataFrame(data_decoded, columns=[f'feature_{i}' for i in range(data_decoded.shape[1])])\n",
    "    return df_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9affb6b0",
   "metadata": {
    "papermill": {
     "duration": 0.005259,
     "end_time": "2024-12-19T08:37:16.674364",
     "exception": false,
     "start_time": "2024-12-19T08:37:16.669105",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822d8215",
   "metadata": {},
   "source": [
    "Realizing feature engineering is an important part, we operate this in order that it enhance the model’s ability to capture patterns in the data, leading to better accuracy, robustness, and interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a37107f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T08:37:16.686697Z",
     "iopub.status.busy": "2024-12-19T08:37:16.686380Z",
     "iopub.status.idle": "2024-12-19T08:38:48.796364Z",
     "shell.execute_reply": "2024-12-19T08:38:48.795526Z"
    },
    "papermill": {
     "duration": 92.11895,
     "end_time": "2024-12-19T08:38:48.798647",
     "exception": false,
     "start_time": "2024-12-19T08:37:16.679697",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 996/996 [01:22<00:00, 12.08it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  6.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.0465, Validation Loss: 0.0366\n",
      "Epoch 2, Train Loss: 0.0246, Validation Loss: 0.0193\n",
      "Epoch 3, Train Loss: 0.0186, Validation Loss: 0.0184\n",
      "Epoch 4, Train Loss: 0.0181, Validation Loss: 0.0182\n",
      "Epoch 5, Train Loss: 0.0181, Validation Loss: 0.0182\n",
      "Epoch 6, Train Loss: 0.0180, Validation Loss: 0.0181\n",
      "Epoch 7, Train Loss: 0.0180, Validation Loss: 0.0181\n",
      "Epoch 8, Train Loss: 0.0179, Validation Loss: 0.0181\n",
      "Epoch 9, Train Loss: 0.0179, Validation Loss: 0.0179\n",
      "Epoch 10, Train Loss: 0.0176, Validation Loss: 0.0174\n",
      "Epoch 11, Train Loss: 0.0162, Validation Loss: 0.0145\n",
      "Epoch 12, Train Loss: 0.0134, Validation Loss: 0.0124\n",
      "Epoch 13, Train Loss: 0.0121, Validation Loss: 0.0115\n",
      "Epoch 14, Train Loss: 0.0114, Validation Loss: 0.0108\n",
      "Epoch 15, Train Loss: 0.0109, Validation Loss: 0.0102\n",
      "Epoch 16, Train Loss: 0.0106, Validation Loss: 0.0100\n",
      "Epoch 17, Train Loss: 0.0104, Validation Loss: 0.0098\n",
      "Epoch 18, Train Loss: 0.0103, Validation Loss: 0.0097\n",
      "Epoch 19, Train Loss: 0.0101, Validation Loss: 0.0097\n",
      "Epoch 20, Train Loss: 0.0101, Validation Loss: 0.0095\n",
      "Epoch 21, Train Loss: 0.0100, Validation Loss: 0.0095\n",
      "Epoch 22, Train Loss: 0.0100, Validation Loss: 0.0095\n",
      "Epoch 23, Train Loss: 0.0099, Validation Loss: 0.0095\n",
      "Epoch 24, Train Loss: 0.0099, Validation Loss: 0.0093\n",
      "Epoch 25, Train Loss: 0.0098, Validation Loss: 0.0092\n",
      "Epoch 26, Train Loss: 0.0095, Validation Loss: 0.0088\n",
      "Epoch 27, Train Loss: 0.0084, Validation Loss: 0.0066\n",
      "Epoch 28, Train Loss: 0.0063, Validation Loss: 0.0057\n",
      "Epoch 29, Train Loss: 0.0059, Validation Loss: 0.0054\n",
      "Epoch 30, Train Loss: 0.0058, Validation Loss: 0.0054\n",
      "Epoch 31, Train Loss: 0.0057, Validation Loss: 0.0052\n",
      "Epoch 32, Train Loss: 0.0057, Validation Loss: 0.0052\n",
      "Epoch 33, Train Loss: 0.0057, Validation Loss: 0.0053\n",
      "Epoch 34, Train Loss: 0.0056, Validation Loss: 0.0052\n",
      "Epoch 35, Train Loss: 0.0056, Validation Loss: 0.0052\n",
      "Epoch 36, Train Loss: 0.0056, Validation Loss: 0.0052\n",
      "Epoch 37, Train Loss: 0.0055, Validation Loss: 0.0052\n",
      "Epoch 38, Train Loss: 0.0055, Validation Loss: 0.0051\n",
      "Epoch 39, Train Loss: 0.0055, Validation Loss: 0.0052\n",
      "Epoch 40, Train Loss: 0.0055, Validation Loss: 0.0051\n",
      "Epoch 41, Train Loss: 0.0054, Validation Loss: 0.0051\n",
      "Epoch 42, Train Loss: 0.0054, Validation Loss: 0.0051\n",
      "Epoch 43, Train Loss: 0.0054, Validation Loss: 0.0052\n",
      "Epoch 44, Train Loss: 0.0054, Validation Loss: 0.0051\n",
      "Epoch 45, Train Loss: 0.0054, Validation Loss: 0.0051\n",
      "Epoch 46, Train Loss: 0.0054, Validation Loss: 0.0050\n",
      "Epoch 47, Train Loss: 0.0053, Validation Loss: 0.0050\n",
      "Epoch 48, Train Loss: 0.0053, Validation Loss: 0.0050\n",
      "Epoch 49, Train Loss: 0.0053, Validation Loss: 0.0050\n",
      "Epoch 50, Train Loss: 0.0053, Validation Loss: 0.0050\n",
      "Epoch 51, Train Loss: 0.0053, Validation Loss: 0.0050\n",
      "Epoch 52, Train Loss: 0.0053, Validation Loss: 0.0050\n",
      "Epoch 53, Train Loss: 0.0053, Validation Loss: 0.0050\n",
      "Epoch 54, Train Loss: 0.0053, Validation Loss: 0.0050\n",
      "Epoch 55, Train Loss: 0.0052, Validation Loss: 0.0050\n",
      "Epoch 56, Train Loss: 0.0052, Validation Loss: 0.0050\n",
      "Epoch 57, Train Loss: 0.0052, Validation Loss: 0.0049\n",
      "Epoch 58, Train Loss: 0.0051, Validation Loss: 0.0050\n",
      "Epoch 59, Train Loss: 0.0052, Validation Loss: 0.0051\n",
      "Epoch 60, Train Loss: 0.0051, Validation Loss: 0.0048\n",
      "Epoch 61, Train Loss: 0.0050, Validation Loss: 0.0048\n",
      "Epoch 62, Train Loss: 0.0049, Validation Loss: 0.0046\n",
      "Epoch 63, Train Loss: 0.0048, Validation Loss: 0.0045\n",
      "Epoch 64, Train Loss: 0.0047, Validation Loss: 0.0044\n",
      "Epoch 65, Train Loss: 0.0047, Validation Loss: 0.0044\n",
      "Epoch 66, Train Loss: 0.0046, Validation Loss: 0.0043\n",
      "Epoch 67, Train Loss: 0.0045, Validation Loss: 0.0043\n",
      "Epoch 68, Train Loss: 0.0045, Validation Loss: 0.0042\n",
      "Epoch 69, Train Loss: 0.0045, Validation Loss: 0.0042\n",
      "Epoch 70, Train Loss: 0.0044, Validation Loss: 0.0041\n",
      "Epoch 71, Train Loss: 0.0044, Validation Loss: 0.0043\n",
      "Epoch 72, Train Loss: 0.0044, Validation Loss: 0.0042\n",
      "Epoch 73, Train Loss: 0.0044, Validation Loss: 0.0041\n",
      "Epoch 74, Train Loss: 0.0043, Validation Loss: 0.0040\n",
      "Epoch 75, Train Loss: 0.0043, Validation Loss: 0.0040\n",
      "Epoch 76, Train Loss: 0.0043, Validation Loss: 0.0040\n",
      "Epoch 77, Train Loss: 0.0043, Validation Loss: 0.0041\n",
      "Epoch 78, Train Loss: 0.0042, Validation Loss: 0.0040\n",
      "Epoch 79, Train Loss: 0.0042, Validation Loss: 0.0041\n",
      "Epoch 80, Train Loss: 0.0042, Validation Loss: 0.0040\n",
      "Epoch 81, Train Loss: 0.0042, Validation Loss: 0.0040\n",
      "Epoch 82, Train Loss: 0.0041, Validation Loss: 0.0040\n",
      "Epoch 83, Train Loss: 0.0041, Validation Loss: 0.0039\n",
      "Epoch 84, Train Loss: 0.0041, Validation Loss: 0.0040\n",
      "Epoch 85, Train Loss: 0.0040, Validation Loss: 0.0039\n",
      "Epoch 86, Train Loss: 0.0040, Validation Loss: 0.0039\n",
      "Epoch 87, Train Loss: 0.0040, Validation Loss: 0.0038\n",
      "Epoch 88, Train Loss: 0.0040, Validation Loss: 0.0038\n",
      "Epoch 89, Train Loss: 0.0039, Validation Loss: 0.0038\n",
      "Epoch 90, Train Loss: 0.0039, Validation Loss: 0.0037\n",
      "Epoch 91, Train Loss: 0.0039, Validation Loss: 0.0038\n",
      "Epoch 92, Train Loss: 0.0038, Validation Loss: 0.0038\n",
      "Epoch 93, Train Loss: 0.0038, Validation Loss: 0.0037\n",
      "Epoch 94, Train Loss: 0.0038, Validation Loss: 0.0037\n",
      "Epoch 95, Train Loss: 0.0038, Validation Loss: 0.0036\n",
      "Epoch 96, Train Loss: 0.0038, Validation Loss: 0.0037\n",
      "Epoch 97, Train Loss: 0.0038, Validation Loss: 0.0037\n",
      "Epoch 98, Train Loss: 0.0037, Validation Loss: 0.0036\n",
      "Epoch 99, Train Loss: 0.0037, Validation Loss: 0.0036\n",
      "Epoch 100, Train Loss: 0.0037, Validation Loss: 0.0036\n",
      "Epoch 1, Train Loss: 0.1244, Validation Loss: 0.1276\n",
      "Epoch 2, Train Loss: 0.1238, Validation Loss: 0.1280\n",
      "Epoch 3, Train Loss: 0.1232, Validation Loss: 0.1284\n",
      "Epoch 4, Train Loss: 0.1225, Validation Loss: 0.1289\n",
      "Epoch 5, Train Loss: 0.1218, Validation Loss: 0.1294\n",
      "Epoch 6, Train Loss: 0.1211, Validation Loss: 0.1299\n",
      "Epoch 7, Train Loss: 0.1203, Validation Loss: 0.1304\n",
      "Epoch 8, Train Loss: 0.1196, Validation Loss: 0.1309\n",
      "Epoch 9, Train Loss: 0.1188, Validation Loss: 0.1315\n",
      "Epoch 10, Train Loss: 0.1179, Validation Loss: 0.1320\n",
      "Epoch 11, Train Loss: 0.1170, Validation Loss: 0.1327\n",
      "Early stopping at epoch 11\n"
     ]
    }
   ],
   "source": [
    "def feature_engineering(df):\n",
    "    season_cols = [col for col in df.columns if 'Season' in col]\n",
    "    df = df.drop(season_cols, axis=1) \n",
    "    df['BMI_Age'] = df['Physical-BMI'] * df['Basic_Demos-Age']\n",
    "    df['Internet_Hours_Age'] = df['PreInt_EduHx-computerinternet_hoursday'] * df['Basic_Demos-Age']\n",
    "    df['BMI_Internet_Hours'] = df['Physical-BMI'] * df['PreInt_EduHx-computerinternet_hoursday']\n",
    "    df['BFP_BMI'] = df['BIA-BIA_Fat'] / df['BIA-BIA_BMI']\n",
    "    df['FFMI_BFP'] = df['BIA-BIA_FFMI'] / df['BIA-BIA_Fat']\n",
    "    df['FMI_BFP'] = df['BIA-BIA_FMI'] / df['BIA-BIA_Fat']\n",
    "    df['LST_TBW'] = df['BIA-BIA_LST'] / df['BIA-BIA_TBW']\n",
    "    df['BFP_BMR'] = df['BIA-BIA_Fat'] * df['BIA-BIA_BMR']\n",
    "    df['BFP_DEE'] = df['BIA-BIA_Fat'] * df['BIA-BIA_DEE']\n",
    "    df['BMR_Weight'] = df['BIA-BIA_BMR'] / df['Physical-Weight']\n",
    "    df['DEE_Weight'] = df['BIA-BIA_DEE'] / df['Physical-Weight']\n",
    "    df['SMM_Height'] = df['BIA-BIA_SMM'] / df['Physical-Height']\n",
    "    df['Muscle_to_Fat'] = df['BIA-BIA_SMM'] / df['BIA-BIA_FMI']\n",
    "    df['Hydration_Status'] = df['BIA-BIA_TBW'] / df['Physical-Weight']\n",
    "    df['ICW_TBW'] = df['BIA-BIA_ICW'] / df['BIA-BIA_TBW']\n",
    "    df['hoursday_Age'] = df['PreInt_EduHx-computerinternet_hoursday'] / df['BMI_Age']\n",
    "\n",
    "    df['Age_Weight'] = df['Basic_Demos-Age'] * df['Physical-Weight']\n",
    "    df['Sex_BMI'] = df['Basic_Demos-Sex'] * df['Physical-BMI']\n",
    "    df['Sex_HeartRate'] = df['Basic_Demos-Sex'] * df['Physical-HeartRate']\n",
    "    df['Age_WaistCirc'] = df['Basic_Demos-Age'] * df['Physical-Waist_Circumference']\n",
    "    df['BMI_FitnessMaxStage'] = df['Physical-BMI'] * df['Fitness_Endurance-Max_Stage']\n",
    "    df['Weight_GripStrengthDominant'] = df['Physical-Weight'] * df['FGC-FGC_GSD']\n",
    "    df['Weight_GripStrengthNonDominant'] = df['Physical-Weight'] * df['FGC-FGC_GSND']\n",
    "    df['HeartRate_FitnessTime'] = df['Physical-HeartRate'] * (df['Fitness_Endurance-Time_Mins'] + df['Fitness_Endurance-Time_Sec'])\n",
    "    df['Age_PushUp'] = df['Basic_Demos-Age'] * df['FGC-FGC_PU']\n",
    "    df['FFMI_Age'] = df['BIA-BIA_FFMI'] * df['Basic_Demos-Age']\n",
    "    df['InternetUse_SleepDisturbance'] = df['PreInt_EduHx-computerinternet_hoursday'] * df['SDS-SDS_Total_Raw']\n",
    "    df['CGAS_BMI'] = df['CGAS-CGAS_Score'] * df['Physical-BMI']\n",
    "    df['CGAS_FitnessMaxStage'] = df['CGAS-CGAS_Score'] * df['Fitness_Endurance-Max_Stage']\n",
    "    \n",
    "    return df\n",
    "\n",
    "train = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\n",
    "sample = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n",
    "\n",
    "train_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\n",
    "test_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")\n",
    "\n",
    "df_train = train_ts.drop('id', axis=1)\n",
    "df_test = test_ts.drop('id', axis=1)\n",
    "\n",
    "train_ts_encoded = perform_autoencoder(df_train, epochs=100, batch_size=32, learning_rate=0.001, patience=10, use_pca=False, scaler_type='MinMaxScaler', sparsity_weight=1e-5)\n",
    "test_ts_encoded = perform_autoencoder(df_test, epochs=100, batch_size=32, learning_rate=0.001, patience=10, use_pca=False, scaler_type='MinMaxScaler', sparsity_weight=1e-5)\n",
    "\n",
    "time_series_cols = train_ts_encoded.columns.tolist()\n",
    "train_ts_encoded[\"id\"]=train_ts[\"id\"]\n",
    "test_ts_encoded['id']=test_ts[\"id\"]\n",
    "\n",
    "train = pd.merge(train, train_ts_encoded, how=\"left\", on='id')\n",
    "test = pd.merge(test, test_ts_encoded, how=\"left\", on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8abb409d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T08:38:48.835697Z",
     "iopub.status.busy": "2024-12-19T08:38:48.835119Z",
     "iopub.status.idle": "2024-12-19T08:38:48.844051Z",
     "shell.execute_reply": "2024-12-19T08:38:48.843291Z"
    },
    "papermill": {
     "duration": 0.028934,
     "end_time": "2024-12-19T08:38:48.845682",
     "exception": false,
     "start_time": "2024-12-19T08:38:48.816748",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = train.drop('id', axis=1)\n",
    "test_id = test[\"id\"]  # for submit\n",
    "test = test.drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200c7918",
   "metadata": {
    "papermill": {
     "duration": 0.016994,
     "end_time": "2024-12-19T08:38:48.879955",
     "exception": false,
     "start_time": "2024-12-19T08:38:48.862961",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Using imputer to process missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13152904",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T08:38:48.915643Z",
     "iopub.status.busy": "2024-12-19T08:38:48.915120Z",
     "iopub.status.idle": "2024-12-19T08:38:55.974563Z",
     "shell.execute_reply": "2024-12-19T08:38:55.973832Z"
    },
    "papermill": {
     "duration": 7.079385,
     "end_time": "2024-12-19T08:38:55.976498",
     "exception": false,
     "start_time": "2024-12-19T08:38:48.897113",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select numeric columns excluding the 'sii' column\n",
    "numeric_cols = train.select_dtypes(include=['float64', 'int64']).columns\n",
    "numeric_cols = numeric_cols.drop('sii')  # Exclude 'sii' from imputation\n",
    "\n",
    "# Apply KNNImputer to numeric columns (excluding 'sii')\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "imputed_data = imputer.fit_transform(train[numeric_cols])\n",
    "train_imputed = pd.DataFrame(imputed_data, columns=numeric_cols)\n",
    "\n",
    "# Restore the 'sii' column from the original dataset without imputation\n",
    "train_imputed['sii'] = train['sii']\n",
    "\n",
    "# Copy non-numeric columns from the original dataset\n",
    "for col in train.columns:\n",
    "    if col not in numeric_cols and col != 'sii':  # Skip 'sii' as it is already restored\n",
    "        train_imputed[col] = train[col]\n",
    "\n",
    "# Replace the original train dataset with the updated one\n",
    "train = train_imputed\n",
    "\n",
    "train = feature_engineering(train)\n",
    "train = train.dropna(thresh=10, axis=0)\n",
    "test = feature_engineering(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4118f5b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T08:38:56.013402Z",
     "iopub.status.busy": "2024-12-19T08:38:56.013109Z",
     "iopub.status.idle": "2024-12-19T08:38:56.031076Z",
     "shell.execute_reply": "2024-12-19T08:38:56.030130Z"
    },
    "papermill": {
     "duration": 0.0388,
     "end_time": "2024-12-19T08:38:56.033208",
     "exception": false,
     "start_time": "2024-12-19T08:38:55.994408",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "featuresCols = ['Basic_Demos-Age', 'Basic_Demos-Sex',\n",
    "                'CGAS-CGAS_Score', 'Physical-BMI',\n",
    "                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n",
    "                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n",
    "                'Fitness_Endurance-Max_Stage',\n",
    "                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n",
    "                'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n",
    "                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n",
    "                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n",
    "                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone',\n",
    "                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n",
    "                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n",
    "                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n",
    "                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n",
    "                'BIA-BIA_TBW', 'PAQ_A-PAQ_A_Total',\n",
    "                'PAQ_C-PAQ_C_Total', 'SDS-SDS_Total_Raw',\n",
    "                'SDS-SDS_Total_T',\n",
    "                'PreInt_EduHx-computerinternet_hoursday', 'sii', 'BMI_Age','Internet_Hours_Age','BMI_Internet_Hours',\n",
    "                'BFP_BMI', 'FFMI_BFP', 'FMI_BFP', 'LST_TBW', 'BFP_BMR', 'BFP_DEE', 'BMR_Weight', 'DEE_Weight',\n",
    "                'SMM_Height', 'Muscle_to_Fat', 'Hydration_Status', 'ICW_TBW','Age_Weight','Sex_BMI','Sex_HeartRate','Age_WaistCirc','BMI_FitnessMaxStage','Weight_GripStrengthDominant','Weight_GripStrengthNonDominant','HeartRate_FitnessTime',\n",
    "'Age_PushUp','FFMI_Age','InternetUse_SleepDisturbance','CGAS_BMI','CGAS_FitnessMaxStage']\n",
    "\n",
    "featuresCols += time_series_cols\n",
    "\n",
    "train = train[featuresCols]\n",
    "train = train.dropna(subset='sii')\n",
    "\n",
    "featuresCols = ['Basic_Demos-Age', 'Basic_Demos-Sex',\n",
    "                'CGAS-CGAS_Score', 'Physical-BMI',\n",
    "                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n",
    "                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n",
    "                'Fitness_Endurance-Max_Stage',\n",
    "                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n",
    "                'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n",
    "                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n",
    "                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n",
    "                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone',\n",
    "                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n",
    "                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n",
    "                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n",
    "                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n",
    "                'BIA-BIA_TBW', 'PAQ_A-PAQ_A_Total',\n",
    "                'PAQ_C-PAQ_C_Total', 'SDS-SDS_Total_Raw',\n",
    "                'SDS-SDS_Total_T',\n",
    "                'PreInt_EduHx-computerinternet_hoursday', 'BMI_Age','Internet_Hours_Age','BMI_Internet_Hours',\n",
    "                'BFP_BMI', 'FFMI_BFP', 'FMI_BFP', 'LST_TBW', 'BFP_BMR', 'BFP_DEE', 'BMR_Weight', 'DEE_Weight',\n",
    "                'SMM_Height', 'Muscle_to_Fat', 'Hydration_Status', 'ICW_TBW','Age_Weight','Sex_BMI','Sex_HeartRate','Age_WaistCirc','BMI_FitnessMaxStage','Weight_GripStrengthDominant','Weight_GripStrengthNonDominant','HeartRate_FitnessTime',\n",
    "'Age_PushUp','FFMI_Age','InternetUse_SleepDisturbance','CGAS_BMI','CGAS_FitnessMaxStage']\n",
    "\n",
    "featuresCols += time_series_cols\n",
    "test = test[featuresCols]\n",
    "\n",
    "if np.any(np.isinf(train)):\n",
    "    train = train.replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "\n",
    "def threshold_Rounder(oof_non_rounded, thresholds):\n",
    "    return np.where(oof_non_rounded < thresholds[0], 0,\n",
    "                    np.where(oof_non_rounded < thresholds[1], 1,\n",
    "                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n",
    "\n",
    "def evaluate_predictions(thresholds, y_true, oof_non_rounded):\n",
    "    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n",
    "    return -quadratic_weighted_kappa(y_true, rounded_p)\n",
    "\n",
    "train_df_le = train\n",
    "test_df_le = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "421e6602",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T08:38:56.072252Z",
     "iopub.status.busy": "2024-12-19T08:38:56.071967Z",
     "iopub.status.idle": "2024-12-19T08:39:36.965324Z",
     "shell.execute_reply": "2024-12-19T08:39:36.964136Z"
    },
    "papermill": {
     "duration": 40.914896,
     "end_time": "2024-12-19T08:39:36.967761",
     "exception": false,
     "start_time": "2024-12-19T08:38:56.052865",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip -q install /kaggle/input/missforest/pytorch_tabnet-4.1.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b69bce2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T08:39:37.004373Z",
     "iopub.status.busy": "2024-12-19T08:39:37.004055Z",
     "iopub.status.idle": "2024-12-19T08:39:37.020129Z",
     "shell.execute_reply": "2024-12-19T08:39:37.019437Z"
    },
    "papermill": {
     "duration": 0.036203,
     "end_time": "2024-12-19T08:39:37.021821",
     "exception": false,
     "start_time": "2024-12-19T08:39:36.985618",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d68894fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T08:39:37.057661Z",
     "iopub.status.busy": "2024-12-19T08:39:37.057399Z",
     "iopub.status.idle": "2024-12-19T08:40:25.061154Z",
     "shell.execute_reply": "2024-12-19T08:40:25.060181Z"
    },
    "papermill": {
     "duration": 48.024045,
     "end_time": "2024-12-19T08:40:25.063242",
     "exception": false,
     "start_time": "2024-12-19T08:39:37.039197",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Linkage: complete, Best Metric: euclidean, Best Silhouette Score: 0.9266826737160142\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002673 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 37429\n",
      "[LightGBM] [Info] Number of data points in the train set: 2188, number of used features: 172\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Info] Start training from score 0.587751\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[199]\tvalid_0's rmse: 0.641045\tvalid_0's l2: 0.410939\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "LightGBM RMSE: 0.6410451365716814\n",
      "CatBoost RMSE: 0.6498500391619494\n",
      "XGBoost RMSE: 0.6291657134032399\n",
      "epoch 0  | loss: 7.72514 | valid_mse: 2.74577 |  0:00:00s\n",
      "epoch 1  | loss: 5.57803 | valid_mse: 1.37772 |  0:00:00s\n",
      "epoch 2  | loss: 3.35576 | valid_mse: 0.86347 |  0:00:00s\n",
      "epoch 3  | loss: 2.88095 | valid_mse: 0.68262 |  0:00:01s\n",
      "epoch 4  | loss: 2.81675 | valid_mse: 1.14836 |  0:00:01s\n",
      "epoch 5  | loss: 2.21146 | valid_mse: 0.93514 |  0:00:01s\n",
      "epoch 6  | loss: 1.73614 | valid_mse: 1.06855 |  0:00:01s\n",
      "epoch 7  | loss: 1.33637 | valid_mse: 1.14297 |  0:00:01s\n",
      "epoch 8  | loss: 1.29756 | valid_mse: 1.8591  |  0:00:01s\n",
      "epoch 9  | loss: 1.23274 | valid_mse: 0.86878 |  0:00:01s\n",
      "epoch 10 | loss: 1.08732 | valid_mse: 1.12774 |  0:00:02s\n",
      "epoch 11 | loss: 1.11214 | valid_mse: 0.92416 |  0:00:02s\n",
      "epoch 12 | loss: 0.91269 | valid_mse: 0.88081 |  0:00:02s\n",
      "epoch 13 | loss: 0.99201 | valid_mse: 1.19322 |  0:00:02s\n",
      "epoch 14 | loss: 0.98901 | valid_mse: 1.03454 |  0:00:02s\n",
      "epoch 15 | loss: 0.885   | valid_mse: 0.85842 |  0:00:02s\n",
      "epoch 16 | loss: 0.86197 | valid_mse: 0.80946 |  0:00:02s\n",
      "epoch 17 | loss: 0.81446 | valid_mse: 0.73048 |  0:00:03s\n",
      "epoch 18 | loss: 0.72647 | valid_mse: 0.71834 |  0:00:03s\n",
      "epoch 19 | loss: 0.74122 | valid_mse: 0.70921 |  0:00:03s\n",
      "epoch 20 | loss: 0.84295 | valid_mse: 0.65281 |  0:00:03s\n",
      "epoch 21 | loss: 0.7233  | valid_mse: 0.80807 |  0:00:03s\n",
      "epoch 22 | loss: 0.85533 | valid_mse: 0.75876 |  0:00:03s\n",
      "epoch 23 | loss: 0.79945 | valid_mse: 0.73222 |  0:00:03s\n",
      "epoch 24 | loss: 0.70438 | valid_mse: 0.96818 |  0:00:04s\n",
      "epoch 25 | loss: 0.80936 | valid_mse: 0.69522 |  0:00:04s\n",
      "epoch 26 | loss: 0.64557 | valid_mse: 0.78502 |  0:00:04s\n",
      "epoch 27 | loss: 0.66183 | valid_mse: 0.63456 |  0:00:04s\n",
      "epoch 28 | loss: 0.61382 | valid_mse: 0.66695 |  0:00:04s\n",
      "epoch 29 | loss: 0.63794 | valid_mse: 0.60771 |  0:00:04s\n",
      "epoch 30 | loss: 0.54771 | valid_mse: 0.66183 |  0:00:04s\n",
      "epoch 31 | loss: 0.62309 | valid_mse: 0.59968 |  0:00:05s\n",
      "epoch 32 | loss: 0.55863 | valid_mse: 0.57789 |  0:00:05s\n",
      "epoch 33 | loss: 0.56416 | valid_mse: 0.71393 |  0:00:05s\n",
      "epoch 34 | loss: 0.53454 | valid_mse: 0.56772 |  0:00:05s\n",
      "epoch 35 | loss: 0.52916 | valid_mse: 0.52171 |  0:00:05s\n",
      "epoch 36 | loss: 0.53952 | valid_mse: 0.51124 |  0:00:05s\n",
      "epoch 37 | loss: 0.51698 | valid_mse: 0.55541 |  0:00:05s\n",
      "epoch 38 | loss: 0.51768 | valid_mse: 0.58034 |  0:00:06s\n",
      "epoch 39 | loss: 0.54775 | valid_mse: 0.52017 |  0:00:06s\n",
      "epoch 40 | loss: 0.52868 | valid_mse: 0.53311 |  0:00:06s\n",
      "epoch 41 | loss: 0.54182 | valid_mse: 0.53514 |  0:00:06s\n",
      "epoch 42 | loss: 0.51541 | valid_mse: 0.52952 |  0:00:06s\n",
      "epoch 43 | loss: 0.52543 | valid_mse: 0.49947 |  0:00:06s\n",
      "epoch 44 | loss: 0.55352 | valid_mse: 0.46951 |  0:00:07s\n",
      "epoch 45 | loss: 0.52854 | valid_mse: 0.50296 |  0:00:07s\n",
      "epoch 46 | loss: 0.54632 | valid_mse: 0.49569 |  0:00:07s\n",
      "epoch 47 | loss: 0.52957 | valid_mse: 0.48594 |  0:00:07s\n",
      "epoch 48 | loss: 0.52951 | valid_mse: 0.50316 |  0:00:07s\n",
      "epoch 49 | loss: 0.48909 | valid_mse: 0.48808 |  0:00:08s\n",
      "epoch 50 | loss: 0.4957  | valid_mse: 0.51791 |  0:00:08s\n",
      "epoch 51 | loss: 0.48586 | valid_mse: 0.50782 |  0:00:08s\n",
      "epoch 52 | loss: 0.49564 | valid_mse: 0.51826 |  0:00:08s\n",
      "epoch 53 | loss: 0.48952 | valid_mse: 0.53695 |  0:00:08s\n",
      "epoch 54 | loss: 0.48654 | valid_mse: 0.5113  |  0:00:08s\n",
      "epoch 55 | loss: 0.47341 | valid_mse: 0.48968 |  0:00:08s\n",
      "epoch 56 | loss: 0.48632 | valid_mse: 0.48746 |  0:00:09s\n",
      "epoch 57 | loss: 0.4803  | valid_mse: 0.49481 |  0:00:09s\n",
      "epoch 58 | loss: 0.47499 | valid_mse: 0.49536 |  0:00:09s\n",
      "epoch 59 | loss: 0.46562 | valid_mse: 0.48836 |  0:00:09s\n",
      "epoch 60 | loss: 0.46985 | valid_mse: 0.49096 |  0:00:09s\n",
      "epoch 61 | loss: 0.47146 | valid_mse: 0.50834 |  0:00:09s\n",
      "epoch 62 | loss: 0.47233 | valid_mse: 0.50317 |  0:00:09s\n",
      "epoch 63 | loss: 0.46031 | valid_mse: 0.49315 |  0:00:10s\n",
      "epoch 64 | loss: 0.46714 | valid_mse: 0.47716 |  0:00:10s\n",
      "epoch 65 | loss: 0.45684 | valid_mse: 0.48052 |  0:00:10s\n",
      "epoch 66 | loss: 0.46513 | valid_mse: 0.48194 |  0:00:10s\n",
      "epoch 67 | loss: 0.44782 | valid_mse: 0.47693 |  0:00:10s\n",
      "epoch 68 | loss: 0.44574 | valid_mse: 0.47058 |  0:00:10s\n",
      "epoch 69 | loss: 0.44389 | valid_mse: 0.46531 |  0:00:10s\n",
      "epoch 70 | loss: 0.44595 | valid_mse: 0.46313 |  0:00:11s\n",
      "epoch 71 | loss: 0.45529 | valid_mse: 0.46628 |  0:00:11s\n",
      "epoch 72 | loss: 0.45224 | valid_mse: 0.46861 |  0:00:11s\n",
      "epoch 73 | loss: 0.44811 | valid_mse: 0.46864 |  0:00:11s\n",
      "epoch 74 | loss: 0.4545  | valid_mse: 0.46887 |  0:00:11s\n",
      "epoch 75 | loss: 0.43841 | valid_mse: 0.46379 |  0:00:11s\n",
      "epoch 76 | loss: 0.44467 | valid_mse: 0.4615  |  0:00:11s\n",
      "epoch 77 | loss: 0.43733 | valid_mse: 0.46152 |  0:00:12s\n",
      "epoch 78 | loss: 0.43974 | valid_mse: 0.46375 |  0:00:12s\n",
      "epoch 79 | loss: 0.45253 | valid_mse: 0.46778 |  0:00:12s\n",
      "epoch 80 | loss: 0.44342 | valid_mse: 0.4676  |  0:00:12s\n",
      "epoch 81 | loss: 0.44226 | valid_mse: 0.46981 |  0:00:12s\n",
      "epoch 82 | loss: 0.43923 | valid_mse: 0.46948 |  0:00:12s\n",
      "epoch 83 | loss: 0.44106 | valid_mse: 0.47115 |  0:00:12s\n",
      "epoch 84 | loss: 0.44441 | valid_mse: 0.47439 |  0:00:13s\n",
      "epoch 85 | loss: 0.42857 | valid_mse: 0.47388 |  0:00:13s\n",
      "epoch 86 | loss: 0.43557 | valid_mse: 0.47717 |  0:00:13s\n",
      "epoch 87 | loss: 0.43643 | valid_mse: 0.48025 |  0:00:13s\n",
      "epoch 88 | loss: 0.43246 | valid_mse: 0.47803 |  0:00:13s\n",
      "epoch 89 | loss: 0.43235 | valid_mse: 0.47755 |  0:00:13s\n",
      "epoch 90 | loss: 0.42754 | valid_mse: 0.47822 |  0:00:13s\n",
      "epoch 91 | loss: 0.42211 | valid_mse: 0.47763 |  0:00:13s\n",
      "epoch 92 | loss: 0.42688 | valid_mse: 0.47832 |  0:00:14s\n",
      "epoch 93 | loss: 0.43257 | valid_mse: 0.47836 |  0:00:14s\n",
      "epoch 94 | loss: 0.43439 | valid_mse: 0.47442 |  0:00:14s\n",
      "epoch 95 | loss: 0.42157 | valid_mse: 0.47421 |  0:00:14s\n",
      "epoch 96 | loss: 0.42649 | valid_mse: 0.47777 |  0:00:14s\n",
      "epoch 97 | loss: 0.42054 | valid_mse: 0.47677 |  0:00:14s\n",
      "epoch 98 | loss: 0.41803 | valid_mse: 0.47668 |  0:00:14s\n",
      "epoch 99 | loss: 0.42555 | valid_mse: 0.4761  |  0:00:15s\n",
      "epoch 100| loss: 0.42783 | valid_mse: 0.47561 |  0:00:15s\n",
      "epoch 101| loss: 0.42498 | valid_mse: 0.47714 |  0:00:15s\n",
      "epoch 102| loss: 0.42076 | valid_mse: 0.47877 |  0:00:15s\n",
      "epoch 103| loss: 0.41878 | valid_mse: 0.47958 |  0:00:15s\n",
      "epoch 104| loss: 0.42532 | valid_mse: 0.47916 |  0:00:15s\n",
      "epoch 105| loss: 0.4203  | valid_mse: 0.48195 |  0:00:15s\n",
      "epoch 106| loss: 0.42618 | valid_mse: 0.48206 |  0:00:16s\n",
      "epoch 107| loss: 0.41718 | valid_mse: 0.48429 |  0:00:16s\n",
      "epoch 108| loss: 0.42219 | valid_mse: 0.48504 |  0:00:16s\n",
      "epoch 109| loss: 0.42249 | valid_mse: 0.48196 |  0:00:16s\n",
      "epoch 110| loss: 0.42699 | valid_mse: 0.47933 |  0:00:16s\n",
      "epoch 111| loss: 0.42401 | valid_mse: 0.48008 |  0:00:16s\n",
      "epoch 112| loss: 0.41647 | valid_mse: 0.48349 |  0:00:17s\n",
      "epoch 113| loss: 0.42624 | valid_mse: 0.48384 |  0:00:17s\n",
      "epoch 114| loss: 0.41215 | valid_mse: 0.48363 |  0:00:17s\n",
      "epoch 115| loss: 0.4191  | valid_mse: 0.48581 |  0:00:17s\n",
      "epoch 116| loss: 0.41796 | valid_mse: 0.48745 |  0:00:17s\n",
      "epoch 117| loss: 0.4144  | valid_mse: 0.48527 |  0:00:17s\n",
      "epoch 118| loss: 0.42175 | valid_mse: 0.48298 |  0:00:18s\n",
      "epoch 119| loss: 0.4196  | valid_mse: 0.48195 |  0:00:18s\n",
      "epoch 120| loss: 0.41429 | valid_mse: 0.48513 |  0:00:18s\n",
      "epoch 121| loss: 0.41853 | valid_mse: 0.48538 |  0:00:18s\n",
      "epoch 122| loss: 0.41139 | valid_mse: 0.4888  |  0:00:18s\n",
      "epoch 123| loss: 0.41801 | valid_mse: 0.489   |  0:00:18s\n",
      "epoch 124| loss: 0.4187  | valid_mse: 0.48648 |  0:00:18s\n",
      "epoch 125| loss: 0.42659 | valid_mse: 0.48591 |  0:00:19s\n",
      "epoch 126| loss: 0.41721 | valid_mse: 0.4856  |  0:00:19s\n",
      "\n",
      "Early stopping occurred at epoch 126 with best_epoch = 76 and best_valid_mse = 0.4615\n",
      "TabNet RMSE: 0.7255167979409685\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002517 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 37429\n",
      "[LightGBM] [Info] Number of data points in the train set: 2188, number of used features: 172\n",
      "[LightGBM] [Info] Start training from score 0.587751\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "epoch 0  | loss: 0.89677 | valid_mse: 0.75385 |  0:00:00s\n",
      "epoch 1  | loss: 0.77338 | valid_mse: 0.69318 |  0:00:00s\n",
      "epoch 2  | loss: 0.69542 | valid_mse: 0.6434  |  0:00:00s\n",
      "epoch 3  | loss: 0.65548 | valid_mse: 0.61634 |  0:00:00s\n",
      "epoch 4  | loss: 0.64835 | valid_mse: 0.59513 |  0:00:00s\n",
      "epoch 5  | loss: 0.62588 | valid_mse: 0.58832 |  0:00:00s\n",
      "epoch 6  | loss: 0.60082 | valid_mse: 0.56517 |  0:00:00s\n",
      "epoch 7  | loss: 0.58984 | valid_mse: 0.56033 |  0:00:00s\n",
      "epoch 8  | loss: 0.58073 | valid_mse: 0.56796 |  0:00:01s\n",
      "epoch 9  | loss: 0.56844 | valid_mse: 0.55895 |  0:00:01s\n",
      "epoch 10 | loss: 0.56144 | valid_mse: 0.54608 |  0:00:01s\n",
      "epoch 11 | loss: 0.5606  | valid_mse: 0.52458 |  0:00:01s\n",
      "epoch 12 | loss: 0.54755 | valid_mse: 0.53    |  0:00:01s\n",
      "epoch 13 | loss: 0.54348 | valid_mse: 0.54193 |  0:00:01s\n",
      "epoch 14 | loss: 0.53294 | valid_mse: 0.53917 |  0:00:01s\n",
      "epoch 15 | loss: 0.5218  | valid_mse: 0.54727 |  0:00:01s\n",
      "epoch 16 | loss: 0.52277 | valid_mse: 0.54453 |  0:00:01s\n",
      "epoch 17 | loss: 0.51642 | valid_mse: 0.53504 |  0:00:02s\n",
      "epoch 18 | loss: 0.51427 | valid_mse: 0.53039 |  0:00:02s\n",
      "epoch 19 | loss: 0.5121  | valid_mse: 0.52901 |  0:00:02s\n",
      "epoch 20 | loss: 0.5178  | valid_mse: 0.53409 |  0:00:02s\n",
      "epoch 21 | loss: 0.50789 | valid_mse: 0.51123 |  0:00:02s\n",
      "epoch 22 | loss: 0.508   | valid_mse: 0.50319 |  0:00:02s\n",
      "epoch 23 | loss: 0.50359 | valid_mse: 0.50878 |  0:00:02s\n",
      "epoch 24 | loss: 0.49195 | valid_mse: 0.51009 |  0:00:02s\n",
      "epoch 25 | loss: 0.49948 | valid_mse: 0.50769 |  0:00:02s\n",
      "epoch 26 | loss: 0.4936  | valid_mse: 0.49715 |  0:00:02s\n",
      "epoch 27 | loss: 0.50498 | valid_mse: 0.50748 |  0:00:03s\n",
      "epoch 28 | loss: 0.49861 | valid_mse: 0.5056  |  0:00:03s\n",
      "epoch 29 | loss: 0.50099 | valid_mse: 0.49724 |  0:00:03s\n",
      "epoch 30 | loss: 0.48931 | valid_mse: 0.49048 |  0:00:03s\n",
      "epoch 31 | loss: 0.50366 | valid_mse: 0.49344 |  0:00:03s\n",
      "epoch 32 | loss: 0.49563 | valid_mse: 0.49961 |  0:00:03s\n",
      "epoch 33 | loss: 0.50203 | valid_mse: 0.51063 |  0:00:03s\n",
      "epoch 34 | loss: 0.50939 | valid_mse: 0.51317 |  0:00:03s\n",
      "epoch 35 | loss: 0.49397 | valid_mse: 0.51642 |  0:00:03s\n",
      "epoch 36 | loss: 0.49851 | valid_mse: 0.52641 |  0:00:03s\n",
      "epoch 37 | loss: 0.49617 | valid_mse: 0.52367 |  0:00:04s\n",
      "epoch 38 | loss: 0.49273 | valid_mse: 0.52229 |  0:00:04s\n",
      "epoch 39 | loss: 0.47609 | valid_mse: 0.51563 |  0:00:04s\n",
      "epoch 40 | loss: 0.49311 | valid_mse: 0.51098 |  0:00:04s\n",
      "epoch 41 | loss: 0.4734  | valid_mse: 0.50883 |  0:00:04s\n",
      "epoch 42 | loss: 0.48268 | valid_mse: 0.50173 |  0:00:04s\n",
      "epoch 43 | loss: 0.49108 | valid_mse: 0.49859 |  0:00:04s\n",
      "epoch 44 | loss: 0.48037 | valid_mse: 0.50347 |  0:00:04s\n",
      "epoch 45 | loss: 0.48329 | valid_mse: 0.50848 |  0:00:04s\n",
      "epoch 46 | loss: 0.47766 | valid_mse: 0.52607 |  0:00:05s\n",
      "epoch 47 | loss: 0.47826 | valid_mse: 0.51656 |  0:00:05s\n",
      "epoch 48 | loss: 0.47716 | valid_mse: 0.5045  |  0:00:05s\n",
      "epoch 49 | loss: 0.47652 | valid_mse: 0.51046 |  0:00:05s\n",
      "epoch 50 | loss: 0.48665 | valid_mse: 0.51289 |  0:00:05s\n",
      "epoch 51 | loss: 0.47826 | valid_mse: 0.50607 |  0:00:05s\n",
      "epoch 52 | loss: 0.48818 | valid_mse: 0.50933 |  0:00:05s\n",
      "epoch 53 | loss: 0.47545 | valid_mse: 0.51908 |  0:00:05s\n",
      "epoch 54 | loss: 0.48686 | valid_mse: 0.52618 |  0:00:05s\n",
      "epoch 55 | loss: 0.48455 | valid_mse: 0.52813 |  0:00:05s\n",
      "epoch 56 | loss: 0.47815 | valid_mse: 0.52414 |  0:00:06s\n",
      "epoch 57 | loss: 0.48367 | valid_mse: 0.52478 |  0:00:06s\n",
      "epoch 58 | loss: 0.46823 | valid_mse: 0.51602 |  0:00:06s\n",
      "epoch 59 | loss: 0.47133 | valid_mse: 0.50661 |  0:00:06s\n",
      "epoch 60 | loss: 0.47368 | valid_mse: 0.50484 |  0:00:06s\n",
      "epoch 61 | loss: 0.46926 | valid_mse: 0.49624 |  0:00:06s\n",
      "epoch 62 | loss: 0.47311 | valid_mse: 0.49726 |  0:00:06s\n",
      "epoch 63 | loss: 0.46791 | valid_mse: 0.50428 |  0:00:06s\n",
      "epoch 64 | loss: 0.47693 | valid_mse: 0.51795 |  0:00:06s\n",
      "epoch 65 | loss: 0.47205 | valid_mse: 0.52024 |  0:00:07s\n",
      "epoch 66 | loss: 0.46599 | valid_mse: 0.5169  |  0:00:07s\n",
      "epoch 67 | loss: 0.47132 | valid_mse: 0.50866 |  0:00:07s\n",
      "epoch 68 | loss: 0.46452 | valid_mse: 0.50203 |  0:00:07s\n",
      "epoch 69 | loss: 0.47016 | valid_mse: 0.50704 |  0:00:07s\n",
      "epoch 70 | loss: 0.46195 | valid_mse: 0.5133  |  0:00:07s\n",
      "epoch 71 | loss: 0.45688 | valid_mse: 0.51946 |  0:00:07s\n",
      "epoch 72 | loss: 0.45789 | valid_mse: 0.52964 |  0:00:07s\n",
      "epoch 73 | loss: 0.45739 | valid_mse: 0.52562 |  0:00:07s\n",
      "epoch 74 | loss: 0.45874 | valid_mse: 0.51858 |  0:00:07s\n",
      "epoch 75 | loss: 0.4752  | valid_mse: 0.52673 |  0:00:07s\n",
      "epoch 76 | loss: 0.46086 | valid_mse: 0.52761 |  0:00:08s\n",
      "epoch 77 | loss: 0.46167 | valid_mse: 0.52843 |  0:00:08s\n",
      "epoch 78 | loss: 0.45747 | valid_mse: 0.50983 |  0:00:08s\n",
      "epoch 79 | loss: 0.45673 | valid_mse: 0.51377 |  0:00:08s\n",
      "epoch 80 | loss: 0.45103 | valid_mse: 0.51517 |  0:00:08s\n",
      "\n",
      "Early stopping occurred at epoch 80 with best_epoch = 30 and best_valid_mse = 0.49048\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "Voting Regressor RMSE: 0.6403983501827447\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "Final Predictions:\n",
      "   sii\n",
      "0    1\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import mean_squared_error, silhouette_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMRegressor, early_stopping\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import torch\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from pytorch_tabnet.callbacks import Callback\n",
    "import os\n",
    "from sklearn.experimental import enable_iterative_imputer  # Required for older scikit-learn versions\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# Model parameters\n",
    "LGBM_Params = {\n",
    "    'learning_rate': 0.046,\n",
    "    'max_depth': 12,\n",
    "    'num_leaves': 478,\n",
    "    'min_data_in_leaf': 13,\n",
    "    'feature_fraction': 0.893,\n",
    "    'bagging_fraction': 0.784,\n",
    "    'bagging_freq': 4,\n",
    "    'lambda_l1': 10,\n",
    "    'lambda_l2': 0.01,\n",
    "    'random_state': 42,\n",
    "    'n_estimators': 200\n",
    "}\n",
    "\n",
    "XGB_Params = {\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 6,\n",
    "    'n_estimators': 200,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'reg_alpha': 1,\n",
    "    'reg_lambda': 5,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "CatBoost_Params = {\n",
    "    'learning_rate': 0.05,\n",
    "    'depth': 6,\n",
    "    'iterations': 200,\n",
    "    'random_seed': 42,\n",
    "    'verbose': 0,\n",
    "    'l2_leaf_reg': 10\n",
    "}\n",
    "\n",
    "TabNet_Params = {\n",
    "    'n_d': 64,\n",
    "    'n_a': 64,\n",
    "    'n_steps': 5,\n",
    "    'gamma': 1.5,\n",
    "    'n_independent': 2,\n",
    "    'n_shared': 2,\n",
    "    'lambda_sparse': 1e-4,\n",
    "    'optimizer_fn': torch.optim.Adam,\n",
    "    'optimizer_params': dict(lr=2e-2, weight_decay=1e-5),\n",
    "    'mask_type': 'entmax',\n",
    "    'scheduler_params': dict(mode=\"min\", patience=10, min_lr=1e-5, factor=0.5),\n",
    "    'scheduler_fn': torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "    'verbose': 1,\n",
    "    'device_name': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "}\n",
    "\n",
    "class TabNetWrapper(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.model = TabNetRegressor(**kwargs)\n",
    "        self.kwargs = kwargs\n",
    "        self.imputer = SimpleImputer(strategy='median')\n",
    "        self.best_model_path = 'best_tabnet_model.pt'\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X_imputed = self.imputer.fit_transform(X)\n",
    "\n",
    "        if hasattr(y, 'values'):\n",
    "            y = y.values\n",
    "\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "            X_imputed, \n",
    "            y, \n",
    "            test_size=0.2,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        self.model.fit(\n",
    "            X_train=X_train,\n",
    "            y_train=y_train.reshape(-1, 1),\n",
    "            eval_set=[(X_valid, y_valid.reshape(-1, 1))],\n",
    "            eval_name=['valid'],\n",
    "            eval_metric=['mse'],\n",
    "            max_epochs=500,\n",
    "            patience=50,\n",
    "            batch_size=1024,\n",
    "            virtual_batch_size=128,\n",
    "            num_workers=0,\n",
    "            drop_last=False,\n",
    "        )\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_imputed = self.imputer.transform(X)\n",
    "        return self.model.predict(X_imputed).flatten()\n",
    "\n",
    "# Preprocessing Function\n",
    "def preprocess_data(train_df_le, test_df_le):\n",
    "    # Save the 'sii' column for train data\n",
    "    train_sii = train_df_le['sii'].copy()\n",
    "\n",
    "    # Find common columns between train and test data (excluding 'sii')\n",
    "    common_columns = train_df_le.columns.difference(['sii']).intersection(test_df_le.columns)\n",
    "    train_df_le = train_df_le[common_columns]\n",
    "    test_df_le = test_df_le[common_columns]\n",
    "\n",
    "    # Impute missing values with mean for both train and test data\n",
    "    preprocessor = SimpleImputer(strategy='mean')\n",
    "    train_df_le_imputed = pd.DataFrame(preprocessor.fit_transform(train_df_le), columns=train_df_le.columns)\n",
    "    test_df_le_imputed = pd.DataFrame(preprocessor.transform(test_df_le), columns=test_df_le.columns)\n",
    "\n",
    "\n",
    "    prepocessor = IterativeImputer(random_state=0)\n",
    "    train_df_le_imputed = pd.DataFrame(preprocessor.fit_transform(train_df_le), columns=train_df_le.columns)\n",
    "    test_df_le_imputed = pd.DataFrame(preprocessor.transform(test_df_le), columns=test_df_le.columns)\n",
    "    \n",
    "    # Standardize numerical columns\n",
    "    scaler = StandardScaler()\n",
    "    numeric_cols = train_df_le_imputed.select_dtypes(include=[np.number]).columns\n",
    "    train_df_le_imputed[numeric_cols] = scaler.fit_transform(train_df_le_imputed[numeric_cols])\n",
    "    test_df_le_imputed[numeric_cols] = scaler.transform(test_df_le_imputed[numeric_cols])\n",
    "\n",
    "    return train_df_le_imputed, test_df_le_imputed, train_sii\n",
    "\n",
    "# Agglomerative Clustering with Parameter Tuning\n",
    "def perform_clustering(train_df_le_imputed):\n",
    "    best_score = -1\n",
    "    best_params = None\n",
    "    linkage_methods = ['ward', 'complete', 'average']\n",
    "    metrics = ['euclidean', 'manhattan', 'cosine']\n",
    "\n",
    "    for linkage_method in linkage_methods:\n",
    "        for metric in metrics:\n",
    "            if linkage_method == 'ward' and metric != 'euclidean':\n",
    "                continue  # Ward linkage only supports euclidean distance\n",
    "            try:\n",
    "                clustering = AgglomerativeClustering(linkage=linkage_method, affinity=metric)\n",
    "                labels = clustering.fit_predict(train_df_le_imputed)\n",
    "                silhouette_avg = silhouette_score(train_df_le_imputed, labels)\n",
    "\n",
    "                if silhouette_avg > best_score:\n",
    "                    best_score = silhouette_avg\n",
    "                    best_params = (linkage_method, metric)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Failed for linkage={linkage_method}, metric={metric}. Error: {e}\")\n",
    "\n",
    "    if best_params is None:\n",
    "        raise ValueError(\"No valid clustering configuration found.\")\n",
    "\n",
    "    print(f\"\\nBest Linkage: {best_params[0]}, Best Metric: {best_params[1]}, Best Silhouette Score: {best_score}\")\n",
    "\n",
    "    best_clustering = AgglomerativeClustering(linkage=best_params[0], affinity=best_params[1], n_clusters=3)\n",
    "    train_clusters = best_clustering.fit_predict(train_df_le_imputed)\n",
    "    train_df_le_imputed['cluster'] = train_clusters\n",
    "\n",
    "    return train_clusters\n",
    "\n",
    "# Predict 'sii'\n",
    "def predict_sii(train_df_le_imputed, train_clusters, train_sii):\n",
    "    X = train_df_le_imputed.copy()\n",
    "    X['cluster'] = train_clusters\n",
    "    y = train_sii\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train LightGBM model\n",
    "    lgbm_model = LGBMRegressor(**LGBM_Params)\n",
    "    lgbm_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], eval_metric='rmse', callbacks=[early_stopping(50)])\n",
    "    rmse_lgbm = np.sqrt(mean_squared_error(y_val, lgbm_model.predict(X_val)))\n",
    "    print(f\"LightGBM RMSE: {rmse_lgbm}\")\n",
    "\n",
    "    # Train CatBoost model\n",
    "    catboost_model = CatBoostRegressor(**CatBoost_Params)\n",
    "    catboost_model.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=50)\n",
    "    rmse_cb = np.sqrt(mean_squared_error(y_val, catboost_model.predict(X_val)))\n",
    "    print(f\"CatBoost RMSE: {rmse_cb}\")\n",
    "\n",
    "    # Train XGBoost model\n",
    "    xgb_model = XGBRegressor(**XGB_Params)\n",
    "    xgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=50, verbose=False)\n",
    "    rmse_xgb = np.sqrt(mean_squared_error(y_val, xgb_model.predict(X_val)))\n",
    "    print(f\"XGBoost RMSE: {rmse_xgb}\")\n",
    "\n",
    "    # Train TabNet model\n",
    "    tabnet_model = TabNetWrapper(**TabNet_Params)\n",
    "    tabnet_model.fit(X_train.values, y_train.values)\n",
    "    rmse_tabnet = np.sqrt(mean_squared_error(y_val, tabnet_model.predict(X_val.values)))\n",
    "    print(f\"TabNet RMSE: {rmse_tabnet}\")\n",
    "\n",
    "    # Voting Regressor\n",
    "    voting_regressor = VotingRegressor(\n",
    "        estimators=[\n",
    "            ('lgbm', lgbm_model), \n",
    "            ('catboost', catboost_model), \n",
    "            ('xgb', xgb_model), \n",
    "            ('tabnet', tabnet_model)\n",
    "        ],\n",
    "        weights=[4.0, 4.0, 5.0, 4.0]\n",
    "    )\n",
    "    voting_regressor.fit(X_train, y_train)\n",
    "    rmse_voting = np.sqrt(mean_squared_error(y_val, voting_regressor.predict(X_val)))\n",
    "    print(f\"Voting Regressor RMSE: {rmse_voting}\")\n",
    "\n",
    "    return voting_regressor.predict(X)\n",
    "\n",
    "# Main Execution\n",
    "train_df_le_imputed, test_df_le_imputed, train_sii = preprocess_data(train_df_le, test_df_le)\n",
    "optimal_clusters = perform_clustering(train_df_le_imputed)\n",
    "X_final = predict_sii(train_df_le_imputed, optimal_clusters, train_sii)\n",
    "train_df_le_imputed['sii'] = np.digitize(X_final, bins=[0, 1, 2, 3])\n",
    "X = train_df_le_imputed\n",
    "test_df_le = test_df_le_imputed\n",
    "\n",
    "print(\"Final Predictions:\")\n",
    "print(train_df_le_imputed[['sii']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cda2bc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T08:40:25.112970Z",
     "iopub.status.busy": "2024-12-19T08:40:25.112643Z",
     "iopub.status.idle": "2024-12-19T08:40:25.146338Z",
     "shell.execute_reply": "2024-12-19T08:40:25.145232Z"
    },
    "papermill": {
     "duration": 0.059149,
     "end_time": "2024-12-19T08:40:25.148169",
     "exception": false,
     "start_time": "2024-12-19T08:40:25.089020",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2736 entries, 0 to 2735\n",
      "Columns: 173 entries, Age_PushUp to cluster\n",
      "dtypes: float64(172), int64(1)\n",
      "memory usage: 3.6 MB\n"
     ]
    }
   ],
   "source": [
    "X = pd.DataFrame(X)\n",
    "# 'sii' \n",
    "y = X['sii']\n",
    "X = X.drop(columns=['sii'])\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0830e1d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T08:40:25.197991Z",
     "iopub.status.busy": "2024-12-19T08:40:25.197449Z",
     "iopub.status.idle": "2024-12-19T08:40:25.212083Z",
     "shell.execute_reply": "2024-12-19T08:40:25.211304Z"
    },
    "papermill": {
     "duration": 0.041406,
     "end_time": "2024-12-19T08:40:25.214004",
     "exception": false,
     "start_time": "2024-12-19T08:40:25.172598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20 entries, 0 to 19\n",
      "Columns: 172 entries, Age_PushUp to feature_95\n",
      "dtypes: float64(172)\n",
      "memory usage: 27.0 KB\n"
     ]
    }
   ],
   "source": [
    "# Convert numpy.ndarray to pandas DataFrame\n",
    "test_df_le = pd.DataFrame(test_df_le)\n",
    "\n",
    "# Now you can use .info() to inspect the DataFrame\n",
    "test_df_le.info()\n",
    "# test_df_le.columns = X.columns\n",
    "X_test = test_df_le"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c015df",
   "metadata": {
    "papermill": {
     "duration": 0.023421,
     "end_time": "2024-12-19T08:40:25.261836",
     "exception": false,
     "start_time": "2024-12-19T08:40:25.238415",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38932a9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T08:40:25.322710Z",
     "iopub.status.busy": "2024-12-19T08:40:25.322147Z",
     "iopub.status.idle": "2024-12-19T08:40:25.331477Z",
     "shell.execute_reply": "2024-12-19T08:40:25.330532Z"
    },
    "papermill": {
     "duration": 0.043312,
     "end_time": "2024-12-19T08:40:25.333150",
     "exception": false,
     "start_time": "2024-12-19T08:40:25.289838",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def TrainML(model_class, test_data):\n",
    "    X = train.drop(['sii'], axis=1)\n",
    "    y = train['sii']\n",
    "\n",
    "    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    train_S = []\n",
    "    test_S = []\n",
    "    \n",
    "    oof_non_rounded = np.zeros(len(y), dtype=float) \n",
    "    oof_rounded = np.zeros(len(y), dtype=int) \n",
    "    test_preds = np.zeros((len(test_data), n_splits))\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        model = clone(model_class)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_val_pred = model.predict(X_val)\n",
    "\n",
    "        oof_non_rounded[test_idx] = y_val_pred\n",
    "        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n",
    "        oof_rounded[test_idx] = y_val_pred_rounded\n",
    "\n",
    "        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n",
    "        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n",
    "\n",
    "        train_S.append(train_kappa)\n",
    "        test_S.append(val_kappa)\n",
    "        \n",
    "        test_preds[:, fold] = model.predict(test_data)\n",
    "        \n",
    "        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n",
    "    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n",
    "\n",
    "    KappaOPtimizer = minimize(evaluate_predictions,\n",
    "                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n",
    "                              method='Nelder-Mead')\n",
    "    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n",
    "    \n",
    "    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n",
    "    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n",
    "\n",
    "    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n",
    "\n",
    "    tpm = test_preds.mean(axis=1)\n",
    "    tpTuned = threshold_Rounder(tpm, KappaOPtimizer.x)\n",
    "    \n",
    "    submission = pd.DataFrame({\n",
    "        'id': sample['id'],\n",
    "        'sii': tpTuned\n",
    "    })\n",
    "\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6b26b0",
   "metadata": {
    "papermill": {
     "duration": 0.022743,
     "end_time": "2024-12-19T08:40:25.384509",
     "exception": false,
     "start_time": "2024-12-19T08:40:25.361766",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Hyperparameter fine tune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5b9ca6",
   "metadata": {
    "papermill": {
     "duration": 0.022493,
     "end_time": "2024-12-19T08:40:25.429943",
     "exception": false,
     "start_time": "2024-12-19T08:40:25.407450",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248bd35e",
   "metadata": {},
   "source": [
    "We use some technique to find out the best parameter for each model we make use of like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a013e87c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T08:40:25.476753Z",
     "iopub.status.busy": "2024-12-19T08:40:25.476091Z",
     "iopub.status.idle": "2024-12-19T08:40:25.481422Z",
     "shell.execute_reply": "2024-12-19T08:40:25.480707Z"
    },
    "papermill": {
     "duration": 0.030402,
     "end_time": "2024-12-19T08:40:25.482923",
     "exception": false,
     "start_time": "2024-12-19T08:40:25.452521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model parameters for LightGBM\n",
    "LGBM_Params = {\n",
    "    'learning_rate': 0.046,\n",
    "    'max_depth': 12,\n",
    "    'num_leaves': 478,\n",
    "    'min_data_in_leaf': 13,\n",
    "    'feature_fraction': 0.893,\n",
    "    'bagging_fraction': 0.784,\n",
    "    'bagging_freq': 4,\n",
    "    'lambda_l1': 10,  # Increased from 6.59\n",
    "    'lambda_l2': 0.01,  # Increased from 2.68e-06\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "# XGBoost parameters\n",
    "XGB_Params = {\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 6,\n",
    "    'n_estimators': 200,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'reg_alpha': 1,  # Increased from 0.1\n",
    "    'reg_lambda': 5,  # Increased from 1\n",
    "    'random_state': SEED,\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "CatBoost_Params = {\n",
    "    'learning_rate': 0.05,\n",
    "    'depth': 6,\n",
    "    'iterations': 200,\n",
    "    'random_seed': SEED,\n",
    "    'verbose': 0,\n",
    "    'l2_leaf_reg': 10,  # Increase this value\n",
    "    # 'cat_features': cat_c,\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "392b6edb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T08:40:25.529888Z",
     "iopub.status.busy": "2024-12-19T08:40:25.529283Z",
     "iopub.status.idle": "2024-12-19T08:40:25.543367Z",
     "shell.execute_reply": "2024-12-19T08:40:25.542542Z"
    },
    "papermill": {
     "duration": 0.039454,
     "end_time": "2024-12-19T08:40:25.545004",
     "exception": false,
     "start_time": "2024-12-19T08:40:25.505550",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pytorch_TabNet\n",
    "\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_tabnet.callbacks import Callback\n",
    "import os\n",
    "import torch\n",
    "from pytorch_tabnet.callbacks import Callback\n",
    "\n",
    "class TabNetWrapper(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.model = TabNetRegressor(**kwargs)\n",
    "        self.kwargs = kwargs\n",
    "        self.imputer = SimpleImputer(strategy='median')\n",
    "        self.best_model_path = 'best_tabnet_model.pt'\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Handle missing values\n",
    "        X_imputed = self.imputer.fit_transform(X)\n",
    "        \n",
    "        if hasattr(y, 'values'):\n",
    "            y = y.values\n",
    "            \n",
    "        # Create internal validation set\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "            X_imputed, \n",
    "            y, \n",
    "            test_size=0.2,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Train TabNet model\n",
    "        history = self.model.fit(\n",
    "            X_train=X_train,\n",
    "            y_train=y_train.reshape(-1, 1),\n",
    "            eval_set=[(X_valid, y_valid.reshape(-1, 1))],\n",
    "            eval_name=['valid'],\n",
    "            eval_metric=['mse'],\n",
    "            max_epochs=500,\n",
    "            patience=50,\n",
    "            batch_size=1024,\n",
    "            virtual_batch_size=128,\n",
    "            num_workers=0,\n",
    "            drop_last=False,\n",
    "            callbacks=[\n",
    "                TabNetPretrainedModelCheckpoint(\n",
    "                    filepath=self.best_model_path,\n",
    "                    monitor='valid_mse',\n",
    "                    mode='min',\n",
    "                    save_best_only=True,\n",
    "                    verbose=True\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Load the best model\n",
    "        if os.path.exists(self.best_model_path):\n",
    "            self.model.load_model(self.best_model_path)\n",
    "            os.remove(self.best_model_path)  # Remove temporary file\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_imputed = self.imputer.transform(X)\n",
    "        return self.model.predict(X_imputed).flatten()\n",
    "    \n",
    "    def __deepcopy__(self, memo):\n",
    "        # Add deepcopy support for scikit-learn\n",
    "        cls = self.__class__\n",
    "        result = cls.__new__(cls)\n",
    "        memo[id(self)] = result\n",
    "        for k, v in self.__dict__.items():\n",
    "            setattr(result, k, deepcopy(v, memo))\n",
    "        return result\n",
    "\n",
    "# TabNet hyperparameters\n",
    "TabNet_Params = {\n",
    "    'n_d': 64,              # Width of the decision prediction layer\n",
    "    'n_a': 64,              # Width of the attention embedding for each step\n",
    "    'n_steps': 5,           # Number of steps in the architecture\n",
    "    'gamma': 1.5,           # Coefficient for feature selection regularization\n",
    "    'n_independent': 2,     # Number of independent GLU layer in each GLU block\n",
    "    'n_shared': 2,          # Number of shared GLU layer in each GLU block\n",
    "    'lambda_sparse': 1e-4,  # Sparsity regularization\n",
    "    'optimizer_fn': torch.optim.Adam,\n",
    "    'optimizer_params': dict(lr=2e-2, weight_decay=1e-5),\n",
    "    'mask_type': 'entmax',\n",
    "    'scheduler_params': dict(mode=\"min\", patience=10, min_lr=1e-5, factor=0.5),\n",
    "    'scheduler_fn': torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "    'verbose': 1,\n",
    "    'device_name': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "}\n",
    "\n",
    "class TabNetPretrainedModelCheckpoint(Callback):\n",
    "    def __init__(self, filepath, monitor='val_loss', mode='min', \n",
    "                 save_best_only=True, verbose=1):\n",
    "        super().__init__()  # Initialize parent class\n",
    "        self.filepath = filepath\n",
    "        self.monitor = monitor\n",
    "        self.mode = mode\n",
    "        self.save_best_only = save_best_only\n",
    "        self.verbose = verbose\n",
    "        self.best = float('inf') if mode == 'min' else -float('inf')\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.model = self.trainer  # Use trainer itself as model\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is None:\n",
    "            return\n",
    "        \n",
    "        # Check if current metric is better than best\n",
    "        if (self.mode == 'min' and current < self.best) or \\\n",
    "           (self.mode == 'max' and current > self.best):\n",
    "            if self.verbose:\n",
    "                print(f'\\nEpoch {epoch}: {self.monitor} improved from {self.best:.4f} to {current:.4f}')\n",
    "            self.best = current\n",
    "            if self.save_best_only:\n",
    "                self.model.save_model(self.filepath)  # Save the entire model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2f12169",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T08:40:25.591721Z",
     "iopub.status.busy": "2024-12-19T08:40:25.591477Z",
     "iopub.status.idle": "2024-12-19T08:40:25.596386Z",
     "shell.execute_reply": "2024-12-19T08:40:25.595775Z"
    },
    "papermill": {
     "duration": 0.029976,
     "end_time": "2024-12-19T08:40:25.597889",
     "exception": false,
     "start_time": "2024-12-19T08:40:25.567913",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create model instances\n",
    "Light = LGBMRegressor(**LGBM_Params, random_state=SEED, verbose=-1, n_estimators=300)\n",
    "XGB_Model = XGBRegressor(**XGB_Params)\n",
    "CatBoost_Model = CatBoostRegressor(**CatBoost_Params)\n",
    "TabNet_Model = TabNetWrapper(**TabNet_Params) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "064e8ef1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T08:40:25.644383Z",
     "iopub.status.busy": "2024-12-19T08:40:25.644157Z",
     "iopub.status.idle": "2024-12-19T08:42:55.436785Z",
     "shell.execute_reply": "2024-12-19T08:42:55.435912Z"
    },
    "papermill": {
     "duration": 149.818057,
     "end_time": "2024-12-19T08:42:55.438555",
     "exception": false,
     "start_time": "2024-12-19T08:40:25.620498",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Folds: 100%|██████████| 5/5 [02:29<00:00, 29.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Train QWK --> 0.7454\n",
      "Mean Validation QWK ---> 0.4088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----> || Optimized QWK SCORE :: \u001b[36m\u001b[1m 0.493\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sii</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00008ff9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fd460</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00105258</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00115b9f</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0016bb22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>001f3379</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0038ba98</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0068a485</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0069fbed</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0083e397</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0087dd65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>00abe655</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>00ae59c9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>00af6387</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>00bd4359</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>00c0cd71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>00d56d4b</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>00d9913d</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>00e6167c</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>00ebc35d</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  sii\n",
       "0   00008ff9    0\n",
       "1   000fd460    0\n",
       "2   00105258    0\n",
       "3   00115b9f    0\n",
       "4   0016bb22    0\n",
       "5   001f3379    0\n",
       "6   0038ba98    1\n",
       "7   0068a485    0\n",
       "8   0069fbed    1\n",
       "9   0083e397    0\n",
       "10  0087dd65    0\n",
       "11  00abe655    0\n",
       "12  00ae59c9    1\n",
       "13  00af6387    1\n",
       "14  00bd4359    0\n",
       "15  00c0cd71    1\n",
       "16  00d56d4b    0\n",
       "17  00d9913d    0\n",
       "18  00e6167c    0\n",
       "19  00ebc35d    0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_model = VotingRegressor(estimators=[\n",
    "    ('lightgbm', Light),\n",
    "    ('xgboost', XGB_Model),\n",
    "    ('catboost', CatBoost_Model),\n",
    "    ('tabnet', TabNet_Model)\n",
    "],weights=[4.0,4.0,5.0,4.0])\n",
    "\n",
    "Submission1 = TrainML(voting_model, test)\n",
    "\n",
    "Submission1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e55ba31",
   "metadata": {
    "papermill": {
     "duration": 0.023066,
     "end_time": "2024-12-19T08:42:55.485455",
     "exception": false,
     "start_time": "2024-12-19T08:42:55.462389",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94c7b3d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T08:42:55.533525Z",
     "iopub.status.busy": "2024-12-19T08:42:55.533233Z",
     "iopub.status.idle": "2024-12-19T08:44:56.238709Z",
     "shell.execute_reply": "2024-12-19T08:44:56.237922Z"
    },
    "papermill": {
     "duration": 120.731745,
     "end_time": "2024-12-19T08:44:56.240395",
     "exception": false,
     "start_time": "2024-12-19T08:42:55.508650",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Folds: 100%|██████████| 5/5 [00:50<00:00, 10.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Train QWK --> 0.7595\n",
      "Mean Validation QWK ---> 0.3926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----> || Optimized QWK SCORE :: \u001b[36m\u001b[1m 0.457\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sii</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00008ff9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fd460</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00105258</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00115b9f</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0016bb22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>001f3379</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0038ba98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0068a485</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0069fbed</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0083e397</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0087dd65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>00abe655</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>00ae59c9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>00af6387</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>00bd4359</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>00c0cd71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>00d56d4b</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>00d9913d</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>00e6167c</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>00ebc35d</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  sii\n",
       "0   00008ff9    1\n",
       "1   000fd460    0\n",
       "2   00105258    0\n",
       "3   00115b9f    0\n",
       "4   0016bb22    0\n",
       "5   001f3379    1\n",
       "6   0038ba98    0\n",
       "7   0068a485    0\n",
       "8   0069fbed    1\n",
       "9   0083e397    0\n",
       "10  0087dd65    0\n",
       "11  00abe655    0\n",
       "12  00ae59c9    1\n",
       "13  00af6387    1\n",
       "14  00bd4359    1\n",
       "15  00c0cd71    1\n",
       "16  00d56d4b    0\n",
       "17  00d9913d    0\n",
       "18  00e6167c    0\n",
       "19  00ebc35d    1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\n",
    "sample = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n",
    "\n",
    "def process_file(filename, dirname):\n",
    "    df = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))\n",
    "    df.drop('step', axis=1, inplace=True)\n",
    "    return df.describe().values.reshape(-1), filename.split('=')[1]\n",
    "\n",
    "def load_time_series(dirname) -> pd.DataFrame:\n",
    "    ids = os.listdir(dirname)\n",
    "    \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(tqdm(executor.map(lambda fname: process_file(fname, dirname), ids), total=len(ids)))\n",
    "    \n",
    "    stats, indexes = zip(*results)\n",
    "    \n",
    "    df = pd.DataFrame(stats, columns=[f\"stat_{i}\" for i in range(len(stats[0]))])\n",
    "    df['id'] = indexes\n",
    "    return df\n",
    "        \n",
    "train_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\n",
    "test_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")\n",
    "\n",
    "time_series_cols = train_ts.columns.tolist()\n",
    "time_series_cols.remove(\"id\")\n",
    "\n",
    "train = pd.merge(train, train_ts, how=\"left\", on='id')\n",
    "test = pd.merge(test, test_ts, how=\"left\", on='id')\n",
    "\n",
    "train = train.drop('id', axis=1)\n",
    "test = test.drop('id', axis=1)   \n",
    "\n",
    "featuresCols = ['Basic_Demos-Enroll_Season', 'Basic_Demos-Age', 'Basic_Demos-Sex',\n",
    "                'CGAS-Season', 'CGAS-CGAS_Score', 'Physical-Season', 'Physical-BMI',\n",
    "                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n",
    "                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n",
    "                'Fitness_Endurance-Season', 'Fitness_Endurance-Max_Stage',\n",
    "                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n",
    "                'FGC-Season', 'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n",
    "                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n",
    "                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n",
    "                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', 'BIA-Season',\n",
    "                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n",
    "                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n",
    "                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n",
    "                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n",
    "                'BIA-BIA_TBW', 'PAQ_A-Season', 'PAQ_A-PAQ_A_Total', 'PAQ_C-Season',\n",
    "                'PAQ_C-PAQ_C_Total', 'SDS-Season', 'SDS-SDS_Total_Raw',\n",
    "                'SDS-SDS_Total_T', 'PreInt_EduHx-Season',\n",
    "                'PreInt_EduHx-computerinternet_hoursday', 'sii']\n",
    "\n",
    "featuresCols += time_series_cols\n",
    "\n",
    "train = train[featuresCols]\n",
    "train = train.dropna(subset='sii')\n",
    "\n",
    "cat_c = ['Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', \n",
    "          'Fitness_Endurance-Season', 'FGC-Season', 'BIA-Season', \n",
    "          'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season']\n",
    "\n",
    "def update(df):\n",
    "    global cat_c\n",
    "    for c in cat_c: \n",
    "        df[c] = df[c].fillna('Missing')\n",
    "        df[c] = df[c].astype('category')\n",
    "    return df\n",
    "        \n",
    "train = update(train)\n",
    "test = update(test)\n",
    "\n",
    "def create_mapping(column, dataset):\n",
    "    unique_values = dataset[column].unique()\n",
    "    return {value: idx for idx, value in enumerate(unique_values)}\n",
    "\n",
    "for col in cat_c:\n",
    "    mapping = create_mapping(col, train)\n",
    "    mappingTe = create_mapping(col, test)\n",
    "    \n",
    "    train[col] = train[col].replace(mapping).astype(int)\n",
    "    test[col] = test[col].replace(mappingTe).astype(int)\n",
    "\n",
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "\n",
    "def threshold_Rounder(oof_non_rounded, thresholds):\n",
    "    return np.where(oof_non_rounded < thresholds[0], 0,\n",
    "                    np.where(oof_non_rounded < thresholds[1], 1,\n",
    "                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n",
    "\n",
    "def evaluate_predictions(thresholds, y_true, oof_non_rounded):\n",
    "    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n",
    "    return -quadratic_weighted_kappa(y_true, rounded_p)\n",
    "\n",
    "def TrainML(model_class, test_data):\n",
    "    X = train.drop(['sii'], axis=1)\n",
    "    y = train['sii']\n",
    "\n",
    "    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    train_S = []\n",
    "    test_S = []\n",
    "    \n",
    "    oof_non_rounded = np.zeros(len(y), dtype=float) \n",
    "    oof_rounded = np.zeros(len(y), dtype=int) \n",
    "    test_preds = np.zeros((len(test_data), n_splits))\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        model = clone(model_class)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_val_pred = model.predict(X_val)\n",
    "\n",
    "        oof_non_rounded[test_idx] = y_val_pred\n",
    "        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n",
    "        oof_rounded[test_idx] = y_val_pred_rounded\n",
    "\n",
    "        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n",
    "        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n",
    "\n",
    "        train_S.append(train_kappa)\n",
    "        test_S.append(val_kappa)\n",
    "        \n",
    "        test_preds[:, fold] = model.predict(test_data)\n",
    "        \n",
    "        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n",
    "    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n",
    "\n",
    "    KappaOPtimizer = minimize(evaluate_predictions,\n",
    "                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n",
    "                              method='Nelder-Mead')\n",
    "    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n",
    "    thresholds = KappaOPtimizer.x\n",
    "    \n",
    "    oof_tuned = threshold_Rounder(oof_non_rounded, thresholds)\n",
    "    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n",
    "\n",
    "    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n",
    "\n",
    "    fold_weights = [1.25, 1.0, 1.0, 1.0, 1.0]\n",
    "    tpm = test_preds.dot(fold_weights) / np.sum(fold_weights)\n",
    "    tpTuned = threshold_Rounder(tpm, thresholds)\n",
    "    \n",
    "    submission = pd.DataFrame({\n",
    "        'id': sample['id'],\n",
    "        'sii': tpTuned\n",
    "    })\n",
    "\n",
    "    return submission\n",
    "\n",
    "# Model parameters for LightGBM\n",
    "Params = {\n",
    "    'learning_rate': 0.046,\n",
    "    'max_depth': 12,\n",
    "    'num_leaves': 478,\n",
    "    'min_data_in_leaf': 13,\n",
    "    'feature_fraction': 0.893,\n",
    "    'bagging_fraction': 0.784,\n",
    "    'bagging_freq': 4,\n",
    "    'lambda_l1': 10,  # Increased from 6.59\n",
    "    'lambda_l2': 0.01  # Increased from 2.68e-06\n",
    "}\n",
    "\n",
    "\n",
    "# XGBoost parameters\n",
    "XGB_Params = {\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 6,\n",
    "    'n_estimators': 200,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'reg_alpha': 1,  # Increased from 0.1\n",
    "    'reg_lambda': 5,  # Increased from 1\n",
    "    'random_state': SEED\n",
    "}\n",
    "\n",
    "\n",
    "CatBoost_Params = {\n",
    "    'learning_rate': 0.05,\n",
    "    'depth': 6,\n",
    "    'iterations': 200,\n",
    "    'random_seed': SEED,\n",
    "    'cat_features': cat_c,\n",
    "    'verbose': 0,\n",
    "    'l2_leaf_reg': 10  # Increase this value\n",
    "}\n",
    "\n",
    "# Create model instances\n",
    "Light = LGBMRegressor(**Params, random_state=SEED, verbose=-1, n_estimators=300)\n",
    "XGB_Model = XGBRegressor(**XGB_Params)\n",
    "CatBoost_Model = CatBoostRegressor(**CatBoost_Params)\n",
    "\n",
    "# Combine models using Voting Regressor\n",
    "voting_model = VotingRegressor(estimators=[\n",
    "    ('lightgbm', Light),\n",
    "    ('xgboost', XGB_Model),\n",
    "    ('catboost', CatBoost_Model)\n",
    "])\n",
    "\n",
    "# Train the ensemble model\n",
    "Submission2 = TrainML(voting_model, test)\n",
    "\n",
    "Submission2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e957339",
   "metadata": {
    "papermill": {
     "duration": 0.024567,
     "end_time": "2024-12-19T08:44:56.289610",
     "exception": false,
     "start_time": "2024-12-19T08:44:56.265043",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50cbc56f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T08:44:56.337985Z",
     "iopub.status.busy": "2024-12-19T08:44:56.337648Z",
     "iopub.status.idle": "2024-12-19T08:48:05.019109Z",
     "shell.execute_reply": "2024-12-19T08:48:05.018182Z"
    },
    "papermill": {
     "duration": 188.707672,
     "end_time": "2024-12-19T08:48:05.020765",
     "exception": false,
     "start_time": "2024-12-19T08:44:56.313093",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Folds: 100%|██████████| 5/5 [01:58<00:00, 23.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Train QWK --> 0.9175\n",
      "Mean Validation QWK ---> 0.3803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----> || Optimized QWK SCORE :: \u001b[36m\u001b[1m 0.450\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sii</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00008ff9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fd460</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00105258</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00115b9f</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0016bb22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>001f3379</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0038ba98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0068a485</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0069fbed</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0083e397</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0087dd65</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>00abe655</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>00ae59c9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>00af6387</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>00bd4359</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>00c0cd71</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>00d56d4b</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>00d9913d</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>00e6167c</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>00ebc35d</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  sii\n",
       "0   00008ff9    2\n",
       "1   000fd460    0\n",
       "2   00105258    0\n",
       "3   00115b9f    0\n",
       "4   0016bb22    1\n",
       "5   001f3379    1\n",
       "6   0038ba98    0\n",
       "7   0068a485    0\n",
       "8   0069fbed    2\n",
       "9   0083e397    0\n",
       "10  0087dd65    1\n",
       "11  00abe655    0\n",
       "12  00ae59c9    2\n",
       "13  00af6387    1\n",
       "14  00bd4359    2\n",
       "15  00c0cd71    2\n",
       "16  00d56d4b    0\n",
       "17  00d9913d    0\n",
       "18  00e6167c    0\n",
       "19  00ebc35d    1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\n",
    "sample = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n",
    "\n",
    "featuresCols = ['Basic_Demos-Enroll_Season', 'Basic_Demos-Age', 'Basic_Demos-Sex',\n",
    "                'CGAS-Season', 'CGAS-CGAS_Score', 'Physical-Season', 'Physical-BMI',\n",
    "                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n",
    "                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n",
    "                'Fitness_Endurance-Season', 'Fitness_Endurance-Max_Stage',\n",
    "                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n",
    "                'FGC-Season', 'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n",
    "                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n",
    "                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n",
    "                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', 'BIA-Season',\n",
    "                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n",
    "                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n",
    "                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n",
    "                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n",
    "                'BIA-BIA_TBW', 'PAQ_A-Season', 'PAQ_A-PAQ_A_Total', 'PAQ_C-Season',\n",
    "                'PAQ_C-PAQ_C_Total', 'SDS-Season', 'SDS-SDS_Total_Raw',\n",
    "                'SDS-SDS_Total_T', 'PreInt_EduHx-Season',\n",
    "                'PreInt_EduHx-computerinternet_hoursday', 'sii']\n",
    "\n",
    "cat_c = ['Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', \n",
    "          'Fitness_Endurance-Season', 'FGC-Season', 'BIA-Season', \n",
    "          'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season']\n",
    "\n",
    "train_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\n",
    "test_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")\n",
    "\n",
    "time_series_cols = train_ts.columns.tolist()\n",
    "time_series_cols.remove(\"id\")\n",
    "\n",
    "train = pd.merge(train, train_ts, how=\"left\", on='id')\n",
    "test = pd.merge(test, test_ts, how=\"left\", on='id')\n",
    "\n",
    "train = train.drop('id', axis=1)\n",
    "test = test.drop('id', axis=1)\n",
    "\n",
    "featuresCols += time_series_cols\n",
    "\n",
    "train = train[featuresCols]\n",
    "train = train.dropna(subset='sii')\n",
    "\n",
    "def update(df):\n",
    "    global cat_c\n",
    "    for c in cat_c: \n",
    "        df[c] = df[c].fillna('Missing')\n",
    "        df[c] = df[c].astype('category')\n",
    "    return df\n",
    "\n",
    "train = update(train)\n",
    "test = update(test)\n",
    "\n",
    "def create_mapping(column, dataset):\n",
    "    unique_values = dataset[column].unique()\n",
    "    return {value: idx for idx, value in enumerate(unique_values)}\n",
    "\n",
    "for col in cat_c:\n",
    "    mapping = create_mapping(col, train)\n",
    "    mappingTe = create_mapping(col, test)\n",
    "    \n",
    "    train[col] = train[col].replace(mapping).astype(int)\n",
    "    test[col] = test[col].replace(mappingTe).astype(int)\n",
    "\n",
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "\n",
    "def threshold_Rounder(oof_non_rounded, thresholds):\n",
    "    return np.where(oof_non_rounded < thresholds[0], 0,\n",
    "                    np.where(oof_non_rounded < thresholds[1], 1,\n",
    "                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n",
    "\n",
    "def evaluate_predictions(thresholds, y_true, oof_non_rounded):\n",
    "    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n",
    "    return -quadratic_weighted_kappa(y_true, rounded_p)\n",
    "\n",
    "def TrainML(model_class, test_data):\n",
    "    X = train.drop(['sii'], axis=1)\n",
    "    y = train['sii']\n",
    "\n",
    "    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    train_S = []\n",
    "    test_S = []\n",
    "    \n",
    "    oof_non_rounded = np.zeros(len(y), dtype=float) \n",
    "    oof_rounded = np.zeros(len(y), dtype=int) \n",
    "    test_preds = np.zeros((len(test_data), n_splits))\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        model = clone(model_class)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_val_pred = model.predict(X_val)\n",
    "\n",
    "        oof_non_rounded[test_idx] = y_val_pred\n",
    "        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n",
    "        oof_rounded[test_idx] = y_val_pred_rounded\n",
    "\n",
    "        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n",
    "        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n",
    "\n",
    "        train_S.append(train_kappa)\n",
    "        test_S.append(val_kappa)\n",
    "        \n",
    "        test_preds[:, fold] = model.predict(test_data)\n",
    "        \n",
    "        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n",
    "    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n",
    "\n",
    "    KappaOPtimizer = minimize(evaluate_predictions,\n",
    "                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n",
    "                              method='Nelder-Mead')\n",
    "    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n",
    "    \n",
    "    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n",
    "    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n",
    "\n",
    "    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n",
    "\n",
    "    tpm = test_preds.mean(axis=1)\n",
    "    tp_rounded = threshold_Rounder(tpm, KappaOPtimizer.x)\n",
    "\n",
    "    return tp_rounded\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "ensemble = VotingRegressor(estimators=[\n",
    "    ('lgb', Pipeline(steps=[('imputer', imputer), ('regressor', LGBMRegressor(random_state=SEED))])),\n",
    "    ('xgb', Pipeline(steps=[('imputer', imputer), ('regressor', XGBRegressor(random_state=SEED))])),\n",
    "    ('cat', Pipeline(steps=[('imputer', imputer), ('regressor', CatBoostRegressor(random_state=SEED, silent=True))])),\n",
    "    ('rf', Pipeline(steps=[('imputer', imputer), ('regressor', RandomForestRegressor(random_state=SEED))])),\n",
    "    ('gb', Pipeline(steps=[('imputer', imputer), ('regressor', GradientBoostingRegressor(random_state=SEED))]))\n",
    "])\n",
    "\n",
    "Submission3 = TrainML(ensemble, test)\n",
    "Submission3 = pd.DataFrame({\n",
    "    'id': sample['id'],\n",
    "    'sii': Submission3\n",
    "})\n",
    "\n",
    "Submission3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206bde4c",
   "metadata": {
    "papermill": {
     "duration": 0.023478,
     "end_time": "2024-12-19T08:48:05.068788",
     "exception": false,
     "start_time": "2024-12-19T08:48:05.045310",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Ensemble 3 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6d1c794",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T08:48:05.117210Z",
     "iopub.status.busy": "2024-12-19T08:48:05.116843Z",
     "iopub.status.idle": "2024-12-19T08:48:05.137663Z",
     "shell.execute_reply": "2024-12-19T08:48:05.136977Z"
    },
    "papermill": {
     "duration": 0.046819,
     "end_time": "2024-12-19T08:48:05.139228",
     "exception": false,
     "start_time": "2024-12-19T08:48:05.092409",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority voting completed and saved to 'Final_Submission.csv'\n"
     ]
    }
   ],
   "source": [
    "sub1 = Submission1\n",
    "sub2 = Submission2\n",
    "sub3 = Submission3\n",
    "\n",
    "sub1 = sub1.sort_values(by='id').reset_index(drop=True)\n",
    "sub2 = sub2.sort_values(by='id').reset_index(drop=True)\n",
    "sub3 = sub3.sort_values(by='id').reset_index(drop=True)\n",
    "\n",
    "combined = pd.DataFrame({\n",
    "    'id': sub1['id'],\n",
    "    'sii_1': sub1['sii'],\n",
    "    'sii_2': sub2['sii'],\n",
    "    'sii_3': sub3['sii'],\n",
    "})\n",
    "\n",
    "def majority_vote(row):\n",
    "    return row.mode()[0]\n",
    "\n",
    "combined['final_sii'] = combined[['sii_1', 'sii_2', 'sii_3']].apply(majority_vote, axis=1)\n",
    "\n",
    "final_submission = combined[['id', 'final_sii']].rename(columns={'final_sii': 'sii'})\n",
    "\n",
    "final_submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Majority voting completed and saved to 'Final_Submission.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38ff0d8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T08:48:05.189564Z",
     "iopub.status.busy": "2024-12-19T08:48:05.189313Z",
     "iopub.status.idle": "2024-12-19T08:48:05.198041Z",
     "shell.execute_reply": "2024-12-19T08:48:05.197123Z"
    },
    "papermill": {
     "duration": 0.035423,
     "end_time": "2024-12-19T08:48:05.199851",
     "exception": false,
     "start_time": "2024-12-19T08:48:05.164428",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sii</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00008ff9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fd460</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00105258</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00115b9f</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0016bb22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>001f3379</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0038ba98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0068a485</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0069fbed</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0083e397</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0087dd65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>00abe655</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>00ae59c9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>00af6387</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>00bd4359</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>00c0cd71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>00d56d4b</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>00d9913d</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>00e6167c</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>00ebc35d</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  sii\n",
       "0   00008ff9    0\n",
       "1   000fd460    0\n",
       "2   00105258    0\n",
       "3   00115b9f    0\n",
       "4   0016bb22    0\n",
       "5   001f3379    1\n",
       "6   0038ba98    0\n",
       "7   0068a485    0\n",
       "8   0069fbed    1\n",
       "9   0083e397    0\n",
       "10  0087dd65    0\n",
       "11  00abe655    0\n",
       "12  00ae59c9    1\n",
       "13  00af6387    1\n",
       "14  00bd4359    0\n",
       "15  00c0cd71    1\n",
       "16  00d56d4b    0\n",
       "17  00d9913d    0\n",
       "18  00e6167c    0\n",
       "19  00ebc35d    1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_submission"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9643020,
     "sourceId": 81933,
     "sourceType": "competition"
    },
    {
     "datasetId": 6316158,
     "sourceId": 10219266,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 679.331631,
   "end_time": "2024-12-19T08:48:09.010494",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-19T08:36:49.678863",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
